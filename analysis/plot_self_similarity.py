"""
Theory Validation Analysis Script - Part 3
==========================================

This script addresses point (2) from the reviewer feedback.

It loads the data generated by `hypo-theory-test.py`
and performs a more rigorous self-similarity analysis. In addition
to visually plotting the collapsed profiles, it quantifies the
collapse by calculating the L2 norm of the difference between
successive rescaled profiles, showing they converge to a single function.
"""

import numpy as np
import matplotlib.pyplot as plt
import os
import glob

# --- Common Setup ---
N_RES = 128
kx_1d, ky_1d, kz_1d = (np.fft.fftfreq(N_RES) * N_RES,)*3
kx, ky, kz = np.meshgrid(kx_1d, ky_1d, kz_1d, indexing='ij')
x_1d = np.linspace(0, 2*np.pi, N_RES, endpoint=False)

def get_vorticity_magnitude(u_hat, v_hat, w_hat):
    omega_x_hat = 1j*ky*w_hat - 1j*kz*v_hat
    omega_y_hat = 1j*kz*u_hat - 1j*kx*w_hat
    omega_z_hat = 1j*kx*v_hat - 1j*ky*u_hat
    omega_x = np.fft.ifftn(omega_x_hat).real
    omega_y = np.fft.ifftn(omega_y_hat).real
    omega_z = np.fft.ifftn(omega_z_hat).real
    return np.sqrt(omega_x**2 + omega_y**2 + omega_z**2)

def analyze_self_similarity_quantitative():
    print("\n--- Running Test 3 (Quantitative): Self-Similarity Analysis ---")
    data_dir = "theory_test_data"
    
    file_pattern = os.path.join(data_dir, "u_hat_*.npy")
    u_files = sorted(glob.glob(file_pattern))
    
    if not u_files:
        print(f"Error: No data files found in '{data_dir}'. Run hypo-theory-test.py first.")
        return

    T_star_str = os.path.basename(u_files[-1]).split('_')[-1].replace('.npy', '')
    T_star = float(T_star_str)
    print(f"Estimated blow-up time T* = {T_star:.5f}")

    fig, axes = plt.subplots(1, 3, figsize=(21, 6))
    fig.suptitle("Quantitative Self-Similarity Analysis", fontsize=16)
    
    # Plot 1: Raw Data
    axes[0].set_title("Raw Vorticity Profiles")
    axes[0].set_xlabel("x")
    axes[0].set_ylabel("Max Vorticity")
    axes[0].grid(True, alpha=0.5)

    # Plot 2: Qualitative Collapse
    axes[1].set_title("Rescaled Profiles (Qualitative Collapse)")
    axes[1].set_xlabel(r"Rescaled distance $(x - x_0) / (T^* - t)^{1/2}$")
    axes[1].set_ylabel(r"Rescaled vorticity $\omega_{max} \cdot (T^* - t)$")
    axes[1].grid(True, alpha=0.5)

    # Plot 3: Quantitative Convergence
    axes[2].set_title("L2 Norm of Profile Differences")
    axes[2].set_xlabel("Time (t)")
    axes[2].set_ylabel(r"$L_2$ Norm of difference")
    axes[2].grid(True, alpha=0.5)
    axes[2].set_yscale('log')

    colors = plt.cm.viridis(np.linspace(0, 1, len(u_files)))
    
    # Storage for rescaled profiles and times
    rescaled_profiles = []
    profile_times = []
    
    # Common grid for interpolation and L2 norm calculation
    common_x_grid = np.linspace(-20, 20, 2048)

    for i, u_file_path in enumerate(u_files):
        time_str = os.path.basename(u_file_path).split('_')[-1].replace('.npy', '')
        t = float(time_str)
        
        v_file_path = os.path.join(data_dir, f"v_hat_t_{time_str}.npy")
        w_file_path = os.path.join(data_dir, f"w_hat_t_{time_str}.npy")
        
        u_hat, v_hat, w_hat = np.load(u_file_path), np.load(v_file_path), np.load(w_file_path)
        
        vort_mag = get_vorticity_magnitude(u_hat, v_hat, w_hat)
        central_plane_vort = vort_mag[:, :, N_RES // 2]
        
        peak_idx = np.unravel_index(np.argmax(central_plane_vort), central_plane_vort.shape)
        x0 = x_1d[peak_idx[0]]
        profile = central_plane_vort[:, peak_idx[1]]
        
        axes[0].plot(x_1d, profile, color=colors[i], label=f"t={t:.5f}")
        
        # --- Rescaling ---
        dt = T_star - t
        if dt < 1e-9: continue
        
        alpha, beta = 1.0, 0.5 # Scaling exponents for 3D Euler/Navier-Stokes
        rescaled_x = (x_1d - x0) / (dt**beta)
        rescaled_vort = profile * (dt**alpha)
        
        axes[1].plot(rescaled_x, rescaled_vort, color=colors[i], label=f"t={t:.5f}")
        
        # Interpolate onto a common grid for comparison
        interp_profile = np.interp(common_x_grid, rescaled_x, rescaled_vort)
        rescaled_profiles.append(interp_profile)
        profile_times.append(t)

    # --- L2 Norm Calculation ---
    l2_diffs = []
    diff_times = []
    for i in range(len(rescaled_profiles) - 1):
        # Calculate L2 norm of the difference between consecutive profiles
        diff = rescaled_profiles[i+1] - rescaled_profiles[i]
        l2_norm = np.sqrt(np.mean(diff**2)) # Use root mean square error for normalization
        l2_diffs.append(l2_norm)
        # Plot the difference at the midpoint time
        diff_times.append((profile_times[i+1] + profile_times[i]) / 2)
        
    if l2_diffs:
        axes[2].plot(diff_times, l2_diffs, 'o-', color='crimson')
        # Optional: fit a line to show the trend
        try:
            m, b = np.polyfit(diff_times, np.log(l2_diffs), 1)
            fit_x = np.array(diff_times)
            fit_y = np.exp(m*fit_x + b)
            axes[2].plot(fit_x, fit_y, '--', color='gray', label=f'Fit (decay rate {abs(m):.2f})')
            axes[2].legend()
        except np.linalg.LinAlgError:
            print("Could not compute trend line for L2 norm.")

    axes[0].legend()
    axes[1].set_xlim(-20, 20)
    axes[1].legend()

    filename = "hypo_theory_test_3_self_similarity_quantitative.png"
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(filename, dpi=200)
    plt.show()
    print(f"Quantitative self-similarity analysis plot saved to {filename}")


if __name__ == "__main__":
    analyze_self_similarity_quantitative() 
